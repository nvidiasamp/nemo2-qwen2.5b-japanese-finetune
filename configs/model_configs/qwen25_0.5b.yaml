# Qwen2.5-0.5B Model Configuration for NeMo 2.0
# Model-specific configuration (not NeMo version specific)
# Updated: Using hf:// protocol for direct model loading

# Model specifications
model:
  _target_: nemo.collections.llm.Qwen2Model
  config:
    hidden_size: 896
    num_layers: 24
    num_attention_heads: 14

# Use HF model ID directly (modern approach - no local conversion needed)
model_path: "hf://Qwen/Qwen2.5-0.5B"

# Path settings in training/inference configuration
data:
  train_path: "data/llm_jp_wiki/nemo_binary/ja_wiki_train_text_document"
  validation_path: "data/llm_jp_wiki/nemo_binary/ja_wiki_val_text_document"

# File structure (for reference only, automatically parsed from HF)
tokenizer:
  _target_: transformers.AutoTokenizer
  pretrained_model_name_or_path: "Qwen/Qwen2.5-0.5B"
  trust_remote_code: true

# Training related configuration
trainer:
  num_nodes: 1
  devices: 1
  max_steps: 1000

# Optimizer configuration
optim:
  lr: 3e-4
  weight_decay: 0.1

# Training strategy
training:
  precision: "bf16-mixed"

# Model configuration
restore:
  path: "hf://Qwen/Qwen2.5-0.5B"  # Use HF model ID directly

# Model vs Framework difference
tokenizer_note: |
  This configuration uses Qwen/Qwen2.5-0.5B tokenizer.
  For Llama models, use "meta-llama/Llama-3.1-8B" instead.
  This is a MODEL difference, not a NeMo Framework version difference.
  Both work in NeMo 2.0 depending on which model architecture you choose.

# Advantages explanation
advantages:
  - "Save local storage space (2.4GB)"
  - "Automatically get latest model version"
  - "Simplified deployment process"
  - "Reduced maintenance cost"
  - "Aligns with modern ML framework practices"

# Important note
note: "This configuration uses the modern hf:// protocol for direct model loading from HuggingFace Hub, eliminating the need for manual import_ckpt conversion steps."

# Reference
reference: "Based on NVIDIA NeMo Framework Japanese continual pre-training guide: https://developer.nvidia.com/ja-jp/blog/how-to-use-continual-pre-training-with-japanese-language-on-nemo-framework/" 