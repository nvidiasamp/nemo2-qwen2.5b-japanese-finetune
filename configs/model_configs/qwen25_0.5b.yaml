# Use direct HuggingFace model ID (modern approach)
# NeMo 2.0 Model Configuration for Qwen2.5-0.5B
# Author: M1nG & Kosuke
# Date: 2025-07-29
# Updated: Using hf:// protocol for direct model loading

# Model specifications
model:
  _target_: nemo.collections.llm.Qwen2Model
  config:
    hidden_size: 896
    num_layers: 24
    num_attention_heads: 14

# Use HF model ID directly (modern approach - no local conversion needed)
model_path: "hf://Qwen/Qwen2.5-0.5B"

# Path settings in training/inference configuration
data:
  train_path: "data/llm_jp_wiki/nemo_binary/ja_wiki_train_text_document"
  validation_path: "data/llm_jp_wiki/nemo_binary/ja_wiki_val_text_document"

# File structure (for reference only, automatically parsed from HF)
tokenizer:
  _target_: transformers.AutoTokenizer
  pretrained_model_name_or_path: "Qwen/Qwen2.5-0.5B"
  trust_remote_code: true

# Training related configuration
trainer:
  num_nodes: 1
  devices: 1
  max_steps: 1000

# Optimizer configuration
optim:
  lr: 3e-4
  weight_decay: 0.1

# Training strategy
training:
  precision: "bf16-mixed"

# Model configuration
restore:
  path: "hf://Qwen/Qwen2.5-0.5B"  # Use HF model ID directly

# Advantages explanation
advantages:
  - "Save local storage space (2.4GB)"
  - "Automatically get latest model version"
  - "Simplified deployment process"
  - "Reduced maintenance cost"
  - "Aligns with modern ML framework practices"

# Important note
note: "This configuration uses the modern hf:// protocol for direct model loading from HuggingFace Hub, eliminating the need for manual import_ckpt conversion steps." 