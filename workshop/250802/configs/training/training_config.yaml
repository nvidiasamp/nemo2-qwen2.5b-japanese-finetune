# 训练配置模板

# 继续学习配置
continual_learning:
  model: "qwen2.5-0.5b"
  max_steps: 1000
  micro_batch_size: 4
  global_batch_size: 64
  seq_length: 2048
  log_every_n_steps: 10

# PEFT配置
peft:
  scheme: "lora"
  max_steps: 500
  micro_batch_size: 4
  global_batch_size: 64
  seq_length: 2048
  log_every_n_steps: 5
  # PEFT特殊配置
  ckpt_async_save: false
  context_parallel_size: 1
  ddp: "megatron"
