# 产品需求文档 (PRD)
# 基于NeMo2.0的Qwen2.5-0.5B日语持续学习与高效微调实践
# Workshop项目：PEFT、SFT、DPO的应用

## 1. 项目概述

### 1.1 项目背景（0.5B模型workshop特化版）
作为NVIDIA学生大使，开展基于NeMo2.0框架的Qwen2.5-0.5B模型日语持续学习与高效微调实践workshop。

**🎯 重要定位**：本项目专门针对0.5B小型模型优化，强调**方法论教学价值**而非大模型级别的效果展示。

### 1.2 项目目标（重新校准）
- **主要目标**: 展示NeMo2.0框架的使用方法和技术流程
- **核心价值**: 0.5B模型的快速迭代和教学友好特性
- **技术演示**: 继续学习→PEFT完整链路实现
- **团队协作**: 与SFT/DPO团队的标准化模型交付
- **WandB集成**: 统一监控和团队协作展示
- **现实期望**: 重视技术方法 > 生成效果质量

### 1.3 目标模型
- **主要模型**: Qwen2.5-0.5B（适合实践教学的小型模型）
- **选择理由**: 在有限计算资源下仍能高效学习
- **定制化**: 使用日语语料库进行持续学习

### 1.4 项目时间线（基于0.5B模型特性优化）
- **项目周期**: 2025年7月9日 - 2025年7月23日 (14天)
- **当前日期**: 2025年7月9日 18:18 JST（项目正式开始！）
- **演示日期**: 2025年7月23日
- **演示时长**: 20分钟（你10分钟 + 队友10分钟）
- **剩余时间**: 14天整

**🎯 基于0.5B模型优化的现实时间估算**：
- **模型导入**: 5-10分钟（一次性操作）
- **继续学习训练**: 1-2小时（0.5B模型训练很快）
- **LoRA微调**: 30-60分钟（参数量小，收敛快）
- **推理演示**: 每次<10秒（0.5B模型推理速度快）
- **重点时间**: 更多时间用于演示准备和方法论展示

## 2. 技术规格

## ✅ **重要纠正：NeMo 2.0的实际使用方式**

**🎯 基于官方文档验证的正确技术路线**：

| 方面 | NeMo 1.0 | NeMo 2.0（官方验证） | 影响 |
|------|----------|---------------------|------|
| **配置方式** | YAML配置文件 | **NeMo-Run + recipe系统** | 完全不同 |
| **执行方式** | 直接脚本执行 | **`run.run()` + `if __name__ == "__main__":`** | 必须更改 |
| **模型配置** | 传统checkpoint | **`llm.qwen25_500m.finetune_recipe()`** | 使用recipe |
| **数据配置** | YAML数据配置 | **`run.Config(DataModule, ...)`** | 对象化配置 |
| **PEFT配置** | YAML中配置LoRA | **recipe中`peft_scheme="lora"`** | 简化但固定 |
| **训练启动** | 直接python命令 | **`run.run(recipe, executor=run.LocalExecutor())`** | 统一执行器 |

**📋 官方文档验证要点**：
- ✅ **Qwen2.5-0.5B支持**: 官方确认支持状态为"Yes"
- ✅ **recipe正确命名**: 使用`llm.qwen25_500m`（官方文档recipe列表验证）
- ✅ **必需的if条件**: `if __name__ == "__main__":` 是官方要求
- ✅ **PEFT特殊配置**: 必需`ckpt_async_save=False`, `ddp="megatron"`, `context_parallel_size=1`
- ✅ **WandB集成**: 通过`lightning.pytorch.loggers.WandbLogger`

**🎯 基于官方验证的正确技术路线**：
- **训练方式**: NeMo-Run + recipe系统（这是唯一正确的方式）
- **模型管理**: 本地下载0.5B模型（约1-2GB）
- **配置方式**: `run.Config` + recipe修改
- **执行方式**: `run.run()` 统一执行
- **推理方式**: `api.generate()` 直接调用

### 2.1 环境要求
- **容器**: NVIDIA NeMo 25.04 Docker容器 (nvcr.io/nvidia/nemo:25.04)
- **GPU**: NVIDIA RTX 6000 Ada Generation（49140MiB显存）
- **框架**: NeMo 2.0 + NeMo-Run（必需）
- **监控工具**: WandB (Weights & Biases)
- **当前状态**: 已下载NeMo 25.04容器（59.7GB）
- **✅ 模型策略**: 本地下载Qwen2.5-0.5B（约1-2GB），便于调试和快速迭代

### 2.2 数据集（基于LLM-JP专业语料库）
- **数据来源**: LLM-JP日语Wikipedia语料库 v3（专业高质量）
  - **数据地址**: https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v3/-/tree/main/ja/ja_wiki
  - **数据机构**: 国立情报学研究所（NII）LLM-JP项目组
  - **数据质量**: 专业清洗和预处理的日语Wikipedia语料
- **所需格式**: NeMo标准二进制格式
  - `train_text_document.bin` / `train_text_document.idx`
  - `validation_text_document.bin` / `validation_text_document.idx`
- **数据规模**: 针对0.5B模型的合理子集（预计1-5GB处理后数据）
- **预处理工具链**: 
  - **主要工具**: NeMo Curator + Uzushio（基于Apache Spark）
  - **处理优势**: GPU加速 + 分布式处理
  - **0.5B优化**: 虽然使用企业级工具，但处理的数据量适中

### 2.3 模型导入与配置 (基于官方文档验证的正确方式)

#### 2.3.1 模型本地下载（官方文档验证）
```python
# ✅ 官方文档验证的正确导入方式
from nemo.collections import llm

# 一次性模型导入到本地（官方文档验证）
if __name__ == "__main__":  # 官方要求必需
    llm.import_ckpt(
        model=llm.Qwen2Model(llm.Qwen25Config500M()),  # 官方文档配置
        source='hf://Qwen/Qwen2.5-0.5B',
        output_path='./models/qwen25_0.5b',
        overwrite=False
    )
```

#### 2.3.2 继续学习配置（基于官方recipe）
```python
# ✅ 基于官方文档验证的继续学习配置
import nemo_run as run
from nemo.collections import llm

# 使用官方验证的0.5B pretraining recipe
continual_recipe = llm.qwen25_500m.pretrain_recipe(  # 官方文档recipe列表验证的正确命名
    dir="./models/checkpoints/qwen25_continual",
    name="japanese_continual_learning",
    num_nodes=1,
    num_gpus_per_node=1,
)

# 配置继续学习的数据（LLM-JP处理后的数据）
continual_recipe.data = run.Config(
    llm.PreTrainingDataModule,
    paths={
        'train': ['./data/llm_jp_wiki/nemo_binary/ja_wiki_train_text_document'],
        'validation': ['./data/llm_jp_wiki/nemo_binary/ja_wiki_validation_text_document'],
        'test': ['./data/llm_jp_wiki/nemo_binary/ja_wiki_validation_text_document']
    },
    seq_length=2048,
    micro_batch_size=4,     # 0.5B模型可以增加batch size
    global_batch_size=64,   # 利用49GB显存优势
    num_workers=4,
    pin_memory=True,
    dataset_kwargs={
        "index_mapping_dir": "./data/llm_jp_wiki/nemo_binary/",
        "data_impl": "mmap"
    }
)

continual_recipe.trainer.max_steps = 1000

# ⚠️ 官方要求：必须在 if __name__ == "__main__": 中执行
if __name__ == "__main__":
    run.run(continual_recipe, executor=run.LocalExecutor())
```

#### 2.3.3 PEFT微调配置（基于官方验证）
```python
# ✅ 基于官方文档验证的PEFT配置
import nemo_run as run
from nemo.collections import llm
from nemo import lightning as nl

# 使用官方验证的0.5B finetune recipe
peft_recipe = llm.qwen25_500m.finetune_recipe(  # 官方文档recipe列表验证的正确命名
    dir="./models/checkpoints/qwen25_peft",
    name="japanese_lora",
    num_nodes=1,
    num_gpus_per_node=1,
    peft_scheme="lora",  # 启用LoRA
)

# 恢复继续学习后的模型
peft_recipe.resume.restore_config = run.Config(
    nl.RestoreConfig,
    path='./models/checkpoints/qwen25_continual'  # 继续学习的checkpoint
)

# 官方验证的关键PEFT配置
peft_recipe.trainer.max_steps = 500
peft_recipe.trainer.strategy.ckpt_async_save = False  # 官方要求：PEFT必需
peft_recipe.trainer.strategy.context_parallel_size = 1  # 官方要求：必需设置为1
peft_recipe.trainer.strategy.ddp = "megatron"  # 官方要求：LoRA/PEFT必需

# ⚠️ 官方要求：必须在 if __name__ == "__main__": 中执行
if __name__ == "__main__":
    run.run(peft_recipe, executor=run.LocalExecutor())
```

## 🔍 **Sample代码分析与官方文档验证总结**

### ✅ **Sample代码完全验证的部分**：
1. **recipe命名**: 使用 `llm.qwen25_500m.finetune_recipe()` (官方文档recipe列表验证)
2. **PEFT特殊配置**: 完全一致的必需配置
   - `ckpt_async_save = False`
   - `context_parallel_size = 1`  
   - `ddp = "megatron"`
3. **MegatronStrategy配置**: 推理配置完全一致
4. **代码结构**: 都使用 `if __name__ == "__main__":` 要求
5. **执行方式**: 通过 `run.run(recipe, executor=run.LocalExecutor())`

### 🔧 **Sample代码的局限性**：
**Sample代码未包含WandB集成** - 这是简化版本，专注于核心PEFT功能演示。

### ✅ **基于官方文档验证的WandB集成**：
官方NeMo 2.0文档明确支持通过 `lightning.pytorch.loggers.WandbLogger` 集成：

```python
# ✅ 官方文档验证的正确WandB集成方式
from lightning.pytorch.loggers import WandbLogger
from nemo.lightning import NeMoLogger

wandb_logger = WandbLogger(
    project="qwen25-japanese-continual",
    name="continual-learning-phase", 
    entity="your-team",
)

nemo_logger = NeMoLogger(
    wandb=wandb_logger,
    log_dir='./logs',
    name='experiment'
)

# 集成到recipe trainer中
recipe.trainer.logger = nemo_logger
```

**📋 结论**: Sample代码提供了核心PEFT实现的solid foundation，PRD中的WandB集成是基于官方文档的正确补充。

**🎯 最终Recipe命名澄清**:
- **Sample代码使用**: `llm.qwen2_7b.finetune_recipe()` (针对7B模型)
- **你的项目应使用**: `llm.qwen25_500m.finetune_recipe()` (针对Qwen2.5-0.5B)
- **命名规律**: `qwen2_xxx` = Qwen2系列, `qwen25_xxx` = Qwen2.5系列
- **官方确认**: recipe列表明确包含 `qwen25_500m`

## 3. 团队职责分工

### 3.1 职责划分
- **继续学习 + PEFT**: 负责人 - 你
  - 日语Wikipedia数据预处理
  - 继续学习训练实现
  - LoRA微调实现
  - WandB监控集成
  
- **SFT（监督微调）**: 其他团队成员
  - 指令跟随数据准备
  - 监督微调实现
  - 性能评估
  
- **DPO（直接偏好优化）**: 其他团队成员
  - 偏好数据准备
  - DPO训练实现
  - 最终模型优化

### 3.2 团队协作接口
- **模型交付标准**: 统一的模型格式和文档规范
- **进度同步**: 共享WandB项目dashboard
- **技术沟通**: 每日同步会议和技术问题讨论
- **演示协调**: 各部分演示内容的整合

## 4. 实施阶段（14天冲刺，基于0.5B模型特性优化）

### 4.1 第一周：核心功能快速实现 (2025/7/9-7/15)

#### Day 1 (7/9): 环境与基础准备（数据预处理并行）
- **任务1.1**: NeMo 2.0容器环境验证（2小时）
- **任务1.2**: LLM-JP数据下载与预处理启动（并行进行5-7小时）
  - 下载LLM-JP Wikipedia v3数据（30-60分钟）
  - 启动Uzushio + NeMo Curator处理流程（后台运行）
- **任务1.3**: Qwen2.5-0.5B模型导入（10分钟）
- **任务1.4**: WandB基础配置（1小时）
- **任务1.5**: 预处理监控与验证（定期检查）

#### Day 2 (7/10): 继续学习验证 
- **任务2.1**: 继续学习训练测试（1-2小时训练）
- **任务2.2**: 基础推理验证（快速测试）
- **任务2.3**: WandB监控集成验证
- **任务2.4**: 训练pipeline调试优化

#### Day 3 (7/11): PEFT-LoRA实现
- **任务3.1**: LoRA配置和测试（30-60分钟训练）
- **任务3.2**: 推理效果对比
- **任务3.3**: 基础演示内容准备
- **任务3.4**: 技术文档记录

#### Day 4-7 (7/12-7/15): 演示优化与内容准备
- **任务4.1**: 多组实验对比（0.5B模型快速迭代优势）
- **任务4.2**: 演示脚本编写和测试
- **任务4.3**: 技术亮点总结
- **任务4.4**: WandB演示面板优化

### 4.2 第二周：演示准备与团队协作 (2025/7/16-7/23)

#### Day 8-10 (7/16-7/18): 团队协作与模型交付
- **任务5.1**: 模型标准化交付给SFT/DPO团队（简化格式）
- **任务5.2**: 技术文档和使用说明编写
- **任务5.3**: WandB权限共享和演示面板准备
- **任务5.4**: 0.5B模型效果边界说明（实事求是）

#### Day 11-12 (7/19-7/20): 演示内容精化
- **任务6.1**: 演示重点确定：方法论 > 模型效果
- **任务6.2**: 10分钟演示脚本完善
- **任务6.3**: 技术亮点提炼（NeMo 2.0使用方法）
- **任务6.4**: 备用演示方案准备

#### Day 13-14 (7/21-7/23): 最终准备与排练
- **任务7.1**: 你的10分钟演示完整排练
- **任务7.2**: 与队友20分钟整体演示协调
- **任务7.3**: 技术问答准备
- **任务7.4**: 演示环境最终测试

## 5. WandB监控与可视化

### 5.1 监控策略 (基于官方文档验证的WandB集成方式)

#### 5.1.1 继续学习监控集成（官方文档验证）
```python
# ✅ 基于官方文档验证的NeMo-Run + WandB集成方式
import nemo_run as run
from nemo.collections import llm
from nemo import lightning as nl
from lightning.pytorch.loggers import WandbLogger  # 官方验证的正确导入

# 配置WandB logger到recipe中（官方验证方式）
def setup_continual_training_with_wandb():
    recipe = llm.qwen25_500m.pretrain_recipe(  # 官方文档recipe列表验证的正确命名
        dir="./models/checkpoints/qwen25_continual",
        name="japanese_continual_learning",
        num_nodes=1,
        num_gpus_per_node=1,
    )
    
    # 官方验证的WandB集成到NeMo trainer中
    wandb_logger = WandbLogger(
        project="qwen25-japanese-continual",
        name="continual-learning-phase",
        entity="your-team",
        tags=["continual-learning", "japanese-wiki", "qwen2.5-0.5b"],
        config={
            "model_size": "0.5B",
            "dataset": "japanese-wikipedia",
            "task": "continual-pretraining"
        }
    )
    
    # 官方文档验证：将logger添加到trainer配置中
    recipe.trainer.logger = wandb_logger
    recipe.trainer.log_every_n_steps = 10
    
    return recipe

if __name__ == "__main__":
    recipe = setup_continual_training_with_wandb()
    run.run(recipe, executor=run.LocalExecutor())
```

#### 5.1.2 PEFT监控集成（官方文档验证）
```python
# ✅ 基于官方文档验证的NeMo-Run + WandB集成到PEFT训练
def setup_peft_training_with_wandb():
    recipe = llm.qwen25_500m.finetune_recipe(  # 官方文档recipe列表验证的正确命名
        dir="./models/checkpoints/qwen25_peft",
        name="japanese_lora",
    num_nodes=1,
    num_gpus_per_node=1,
    peft_scheme="lora",
)
    
    # PEFT专门的WandB配置（官方验证）
    wandb_logger = WandbLogger(
        project="qwen25-japanese-peft",
        name="lora-finetuning-phase", 
        entity="your-team",
        tags=["peft", "lora", "japanese-tuning"],
        config={
            "lora_rank": 16,  # 默认recipe配置
            "lora_alpha": 32,
            "base_model": "qwen2.5-0.5b-continued",
            "peft_scheme": "lora"
        }
    )
    
    # 官方验证：集成logger到PEFT trainer
    recipe.trainer.logger = wandb_logger
    recipe.trainer.log_every_n_steps = 5  # PEFT训练步数较少，更频繁记录
    
    # 官方要求的PEFT特殊配置
    recipe.trainer.strategy.ckpt_async_save = False
    recipe.trainer.strategy.context_parallel_size = 1
    recipe.trainer.strategy.ddp = "megatron"
    
    return recipe

if __name__ == "__main__":
    recipe = setup_peft_training_with_wandb()
    run.run(recipe, executor=run.LocalExecutor())
```

#### 5.1.3 团队协作监控（官方验证配置）
```python
# ✅ 基于官方验证的共享WandB workspace配置
WANDB_CONFIG = {
    "entity": "your-team",  # 团队workspace
    "projects": {
        "continual": "qwen25-japanese-continual",
        "peft": "qwen25-japanese-peft", 
        "sft": "qwen25-japanese-sft",     # 队友SFT项目
        "dpo": "qwen25-japanese-dpo",     # 队友DPO项目
        "demo": "qwen25-demo-live"        # 演示监控
    },
    "tags": ["nemo2.0", "qwen2.5-0.5b", "japanese", "workshop"]
}
```

### 5.2 可视化面板
- **训练监控**: 实时损失曲线、学习率变化、GPU利用率
- **模型对比**: 三阶段模型效果对比（原始/继续学习/LoRA微调）
- **性能分析**: 推理速度、内存使用、日语生成质量
- **团队协作**: 共享进度dashboard和实验结果

### 5.3 演示推理集成 (基于官方文档验证的正确方式)
```python
# ✅ 基于官方文档验证的演示推理
import torch
from megatron.core.inference.common_inference_params import CommonInferenceParams
import nemo.lightning as nl
from nemo.collections.llm import api
import wandb
import time

def setup_demo_inference():
    # 演示监控初始化（官方验证）
    wandb.init(
        project="qwen25-demo-live",
        name="demo-session",
        entity="your-team",
        tags=["demo", "inference", "qwen2.5-0.5b", "lora"],
        config={
            "model": "qwen2.5-0.5b-lora",
            "checkpoint": "./models/checkpoints/qwen25_peft",
            "inference_params": {
                "temperature": 0.1,
                "top_k": 1,
                "num_tokens_to_generate": 512
            }
        }
    )
    
    # 官方验证的NeMo inference配置
    strategy = nl.MegatronStrategy(
        tensor_model_parallel_size=1,
        pipeline_model_parallel_size=1,
        context_parallel_size=1,
        sequence_parallel=False,
        setup_optimizers=False,
    )
    
    trainer = nl.Trainer(
        accelerator="gpu",
        devices=1,
        num_nodes=1,
        strategy=strategy,
        plugins=nl.MegatronMixedPrecision(
            precision="bf16-mixed",
            params_dtype=torch.bfloat16,
            pipeline_dtype=torch.bfloat16,
        ),
    )
    
    return trainer

def demo_generate_with_monitoring(trainer, checkpoint_path, prompts):
    """基于官方文档验证的推理演示"""
    
    # 推理参数配置（官方验证）
    inference_params = CommonInferenceParams(
        temperature=0.1, 
        top_k=1, 
        num_tokens_to_generate=512
    )
    
    demo_results = []
    
    for prompt in prompts:
        start_time = time.time()
        
        # 使用官方验证的NeMo API推理
        results = api.generate(
            path=checkpoint_path,
            prompts=[prompt],
            trainer=trainer,
            inference_params=inference_params,
            text_only=True,
        )
        
        inference_time = time.time() - start_time
        response = results[0] if results else "生成失败"
        
        # WandB实时记录（官方验证方式）
        wandb.log({
            "demo_prompt": prompt,
            "demo_response": response,
            "inference_time": inference_time,
            "response_length": len(response),
            "timestamp": time.time()
        })
        
        demo_results.append({
            "prompt": prompt,
            "response": response,
            "time": inference_time
        })
        
        print(f"Q: {prompt}")
        print(f"A: {response}")
        print(f"Time: {inference_time:.2f}s\n")
    
    return demo_results

# 演示脚本主体（官方要求的结构）
if __name__ == "__main__":
    # 演示用prompts
    demo_prompts = [
        "日本の美しい季節について教えてください。",
        "人工知能の未来はどうなると思いますか？", 
        "美味しい料理のレシピを教えてください。"
    ]
    
    # PEFT模型checkpoint路径（官方验证格式）
    adapter_checkpoint = './models/checkpoints/qwen25_peft/japanese_lora/checkpoints/model_name=0--val_loss=x.xx-step=xxx-consumed_samples=xxx.0-last'
    
    # 设置推理环境（官方验证）
    trainer = setup_demo_inference()
    
    # 执行演示推理
    results = demo_generate_with_monitoring(trainer, adapter_checkpoint, demo_prompts)
    
    wandb.finish()
```

## 6. 现有代码利用

### 6.1 参考代码
- `sample/abeja-qwen-peft-tuning-example.py`: Qwen2.5-7B LoRA实现
- `sample/abeja-qwen-peft-inference.py`: 推理脚本
- `sample/elyza-peft-tuning-example.py`: ELYZA-Llama2-7B实现

### 6.2 修改要点
- 将模型规模从7B调整为0.5B
- 适配日语Wikipedia语料库
- 集成WandB监控功能
- 增加workshop演示功能

## 7. 整体演示结构（20分钟总时长）

### 7.1 演示时间分配
- **Part 1 - 继续学习+PEFT（你）**: 10分钟
- **Part 2 - SFT+DPO（队友）**: 10分钟
- **总时长**: 20分钟

### 7.2 Part 1: NeMo 2.0方法论演示（你的10分钟）

#### 7.2.1 NeMo 2.0技术栈展示（4分钟）
- **Recipe系统演示**（2分钟）：`llm.qwen25_500m.pretrain_recipe()`的使用方法
- **NeMo-Run执行**（1分钟）：`run.run()`统一执行器的优势
- **配置灵活性**（1分钟）：`run.Config`对象化配置的便利性

#### 7.2.2 专业数据处理+0.5B快速迭代（4分钟）
- **LLM-JP数据处理展示**（1分钟）：NeMo Curator + Uzushio企业级工具链
- **训练速度展示**（2分钟）：1-2小时完成继续学习，30分钟完成LoRA
- **资源效率对比**（1分钟）：49GB显存使用<20%，适合教学与研究环境

#### 7.2.3 Workshop教学价值与团队协作（2分钟）
- **教学友好性**（1分钟）：0.5B模型适合实践教学的原因
- **标准化交付**（1分钟）：与SFT/DPO团队的无缝衔接

### 7.3 Part 2: SFT+DPO专项演示（队友的10分钟）
- **SFT实现**（4-5分钟）：监督微调过程和效果
- **DPO实现**（4-5分钟）：偏好优化和最终效果
- **整体总结**（1-2分钟）：项目成果和技术价值

### 7.4 演示协调要点
- **技术衔接**：明确展示模型从你到队友的传递过程
- **WandB统一**：共享监控面板，展示完整流程
- **时间控制**：每部分严格控制在10分钟内
- **备用预案**：各自准备独立的演示环境

## 8. 团队协作接口设计

### 8.1 模型交付标准
```python
# 标准化模型输出
def export_model_for_team(model, stage):
    """
    stage: "continual-learned" | "lora-finetuned"
    """
    output_path = f"./models/qwen25_0.5b_{stage}/"
    
    # 标准格式导出
    model.save_pretrained(output_path)
    
    # 交付文档
    create_model_card(output_path, stage)
    
    # WandB artifact
    wandb.save(output_path)
```

### 8.2 进度同步机制
```python
# 团队进度共享
def update_team_progress(phase, status, metrics):
    wandb.log({
        f"team_progress_{phase}": status,
        f"team_metrics_{phase}": metrics,
        "timestamp": datetime.now()
    })
```

### 8.3 协作清单
**每日团队同步**:
- [ ] 进度更新到共享WandB
- [ ] 模型状态同步
- [ ] 技术问题讨论
- [ ] 下一日计划确认

**演示协调**:
- [ ] 演示时间分配：你10分钟 + 队友10分钟
- [ ] 技术衔接点确认：模型传递展示
- [ ] WandB统一面板：共享监控展示
- [ ] 备用预案：各自独立演示环境

## 9. 关键里程碑检查点

### 9.1 Critical Path（基于0.5B模型优化）
- **Day 1**: 环境验证+模型导入完成（8小时内）
- **Day 2**: 继续学习训练完成（1-2小时训练）
- **Day 3**: LoRA微调完成（30-60分钟训练）
- **Day 4-7**: 演示内容准备和多组实验
- **Day 10**: 模型交付给团队（简化格式）
- **Day 12**: 你的10分钟演示脚本完成
- **Day 13**: 20分钟整体演示协调完成

### 9.2 风险预警与应对（0.5B模型现实版）
- **如果Day 1环境有问题** → 立即使用备用环境，0.5B模型资源需求低
- **如果训练不收敛** → 降低学习率，0.5B模型容易调试
- **如果演示效果不佳** → 强调方法论，淡化生成质量对比

## 10. 风险分析

### 10.1 技术风险
- **内存不足**: 49GB显存对0.5B模型足够，但需调整批处理大小
- **数据预处理**: 日语语料库预处理耗时
- **模型转换**: HuggingFace到NeMo格式转换可能出错
- **团队协作**: 模型交付格式不兼容

### 10.2 缓解措施
- 优化批处理大小和序列长度设置
- 提前完成数据预处理
- 准备多个备用环境
- 制定标准的模型交付格式

## 11. 技术细节

### 11.1 LLM-JP数据预处理管道（NeMo Curator + Uzushio）

#### 11.1.1 环境准备
```bash
# Apache Spark环境设置（NeMo Curator依赖）
export SPARK_HOME=/opt/spark
export PYSPARK_PYTHON=/usr/bin/python3
export PYSPARK_DRIVER_PYTHON=/usr/bin/python3

# Uzushio工具安装
pip install uzushio
pip install nemo-curator[all]
```

#### 11.1.2 LLM-JP数据下载与预处理
```python
# Step 1: 下载LLM-JP Wikipedia v3数据
import os
import subprocess
from pathlib import Path

# 创建数据目录
data_dir = Path("./data/llm_jp_wiki")
data_dir.mkdir(parents=True, exist_ok=True)

# 从LLM-JP GitLab下载（需要访问权限）
llm_jp_repo = "https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v3.git"
subprocess.run([
    "git", "clone", "--depth", "1", 
    "--filter=blob:none", "--sparse-checkout",
    llm_jp_repo, str(data_dir)
])

# 设置sparse-checkout只下载日语wiki部分
subprocess.run([
    "git", "-C", str(data_dir), 
    "sparse-checkout", "set", "ja/ja_wiki"
])

# Step 2: 使用Uzushio进行初步预处理（适合0.5B模型的数据量）
from uzushio.main import main as uzushio_main

uzushio_config = {
    "input_dir": str(data_dir / "ja/ja_wiki"),
    "output_dir": str(data_dir / "processed"),
    "max_docs": 100000,  # 限制文档数量，适合0.5B模型
    "min_text_length": 100,
    "max_text_length": 4096,
    "language": "ja",
    "num_workers": 4
}

# 执行Uzushio预处理
uzushio_main(**uzushio_config)

# Step 3: NeMo Curator转换为NeMo格式
from nemo_curator.main import main as curator_main
from nemo_curator.datasets import DocumentDataset
from nemo_curator.modules import ExactDuplicates, FuzzyDuplicates
from nemo_curator.filters import WordCountFilter, LanguageIdentificationFilter

# 配置NeMo Curator处理
curator_config = {
    "input_data_dir": str(data_dir / "processed"),
    "output_data_dir": str(data_dir / "nemo_format"),
    "input_file_type": "jsonl",
    "output_file_type": "parquet",
    
    # 针对日语的过滤器
    "filters": [
        WordCountFilter(min_words=10, max_words=2048),
        LanguageIdentificationFilter(language="ja", threshold=0.8)
    ],
    
    # 去重处理
    "dedup_modules": [
        ExactDuplicates(),
        FuzzyDuplicates(jaccard_threshold=0.8)
    ],
    
    # 分布式处理配置
    "num_workers": 2,  # 适合单机环境
    "batch_size": 1000
}

# 执行NeMo Curator处理
dataset = DocumentDataset.read_json(
    curator_config["input_data_dir"], 
    backend="cudf"  # GPU加速
)

# 应用过滤器和去重
for filter_module in curator_config["filters"]:
    dataset = filter_module(dataset)

for dedup_module in curator_config["dedup_modules"]:
    dataset = dedup_module(dataset)

# 保存处理后的数据
dataset.to_parquet(curator_config["output_data_dir"])

# Step 4: 转换为NeMo训练格式
from nemo.collections.nlp.data.language_modeling.preprocess_data_for_megatron import main as preprocess_main

# 配置NeMo预处理参数
nemo_preprocess_config = {
    "input": str(data_dir / "nemo_format"),
    "output_prefix": str(data_dir / "nemo_binary" / "ja_wiki"),
    "vocab_file": "path/to/qwen25_tokenizer/vocab.json",  # Qwen2.5 tokenizer
    "dataset_impl": "mmap",
    "tokenizer_type": "HuggingFaceTokenizer",
    "seq_length": 2048,
    "workers": 4,
    "split_sentences": True,
    "keep_newlines": True
}

# 执行预处理，生成 .bin 和 .idx 文件
preprocess_main(**nemo_preprocess_config)

print("数据预处理完成！")
print(f"训练数据: {data_dir}/nemo_binary/ja_wiki_train_text_document.bin/.idx")
print(f"验证数据: {data_dir}/nemo_binary/ja_wiki_validation_text_document.bin/.idx")
```

#### 11.1.3 预处理时间估算（基于0.5B模型需求）
- **数据下载**: 30-60分钟（取决于网络）
- **Uzushio预处理**: 1-2小时（100K文档，适合0.5B模型）
- **NeMo Curator处理**: 2-3小时（GPU加速去重和过滤）
- **NeMo格式转换**: 1小时（生成.bin/.idx文件）
- **总计**: 5-7小时（可在Day 1并行进行）

#### 11.1.4 0.5B模型数据优化策略
```python
# 针对0.5B模型的数据量控制
DATA_OPTIMIZATION = {
    "max_documents": 100000,        # 限制文档数量
    "target_data_size": "1-5GB",    # 处理后数据目标大小
    "sequence_length": 2048,        # 适中序列长度
    "train_split": 0.95,           # 95%训练，5%验证
    "quality_threshold": 0.8,       # 语言识别阈值
    "dedup_threshold": 0.8          # 去重相似度阈值
}
```

### 11.2 训练配置 (基于官方文档验证的正确方式)
```python
# ✅ 基于官方文档验证的NeMo-Run配置方式
import nemo_run as run
from nemo.collections import llm
from nemo import lightning as nl
from lightning.pytorch.loggers import WandbLogger

# 继续学习配置（基于官方recipe系统）
def setup_continual_pretraining():
    # 使用官方0.5B预训练recipe
    recipe = llm.qwen25_500m.pretrain_recipe(
        dir="./models/checkpoints/qwen25_continual",
        name="japanese_continual_learning",
        num_nodes=1,
        num_gpus_per_node=1,
    )
    
    # 配置日语数据（需要自定义DataModule）
    recipe.data = run.Config(
        llm.PreTrainingDataModule,
        paths=["/path/to/japanese_wiki_data"],
        seq_length=2048,
        micro_batch_size=1,
        global_batch_size=8,
    )
    
    # WandB集成
    wandb_logger = WandbLogger(
        project="qwen25-japanese-continual",
        entity="your-team",
        name="continual-learning"
    )
    recipe.trainer.logger = wandb_logger
    recipe.trainer.max_steps = 1000
    recipe.trainer.log_every_n_steps = 10
    
    return recipe

# PEFT-LoRA配置（完全基于sample代码）
def setup_peft_training():
    # 使用官方0.5B微调recipe（参考sample/abeja-qwen-peft-tuning-example.py）
    recipe = llm.qwen25_500m.finetune_recipe(
        dir="./models/checkpoints/qwen25_peft",
        name="japanese_lora",
        num_nodes=1,
        num_gpus_per_node=1,
        peft_scheme="lora",  # 启用LoRA
    )
    
    # 恢复继续学习的checkpoint
    recipe.resume.restore_config = run.Config(
        nl.RestoreConfig,
        path='./models/checkpoints/qwen25_continual'
    )
    
    # 配置自定义日语微调数据（基于LLM-JP数据的LoRA微调子集）
    recipe.data = run.Config(
        JapaneseFineTuningDataModule,  # 基于sample代码的自定义DataModule
        dataset_root='./data/llm_jp_wiki/finetune_subset/',
        data_prefix='./data/llm_jp_wiki/nemo_binary/ja_wiki_validation_text_document',  # 使用验证集作为微调数据
        seq_length=2048,                # 适中长度，0.5B模型不需要4096
        micro_batch_size=4,             # 0.5B模型可以增加
        global_batch_size=64,           # 利用49GB显存
        num_workers=4,
        pin_memory=True
    )
    
    # PEFT关键配置（基于sample代码）
    recipe.trainer.max_steps = 500
    recipe.trainer.num_sanity_val_steps = 0
    recipe.trainer.strategy.ckpt_async_save = False  # PEFT必需
    recipe.trainer.strategy.context_parallel_size = 1  # 必需设置为1
    recipe.trainer.strategy.ddp = "megatron"  # LoRA必需
    recipe.trainer.val_check_interval = 10
    
    # WandB集成
    wandb_logger = WandbLogger(
        project="qwen25-japanese-peft",
        entity="your-team",
        name="lora-finetuning"
    )
    recipe.trainer.logger = wandb_logger
    
    return recipe

# 自定义数据模块（基于LLM-JP数据）
class JapaneseFineTuningDataModule(FineTuningDataModule):
    """基于LLM-JP Wikipedia数据的微调模块，参考sample/abeja-qwen-peft-tuning-example.py"""
    
    def __init__(self, data_prefix, **kwargs):
        super().__init__(**kwargs)
        self.data_prefix = data_prefix
        self.train_ds = self._create_dataset(f"{data_prefix}_train_text_document")
        self.validation_ds = self._create_dataset(f"{data_prefix}_validation_text_document")
    
    def _create_dataset(self, data_path):
        """创建基于LLM-JP二进制数据的数据集"""
        from nemo.collections.nlp.data.language_modeling.megatron_gpt_dataset import GPTDataset
        
        return GPTDataset(
            data_prefix=data_path,
            documents=None,
            indexed_dataset=None,
            num_samples=None,
            seq_length=self.seq_length,
            seed=1234,
            return_doc_ids=False
        )
    
    def form_instruction_response(self, text_segment):
        """将LLM-JP文本格式化为指示-响应格式"""
        # 简单的文本分割策略，将长文本分为问题-答案对
        sentences = text_segment.split('。')
        if len(sentences) >= 2:
            question = sentences[0] + '。'
            response = '。'.join(sentences[1:3]) + '。'
            
            formatted = f"### 指示:\n以下の内容について説明してください。\n"
            formatted += f"### 入力:\n{question}\n"
            formatted += f"### 応答:\n{response}"
            return formatted
        else:
            return text_segment

# 推荐配置参数（0.5B模型优化，资源高效）
CONTINUAL_LEARNING_CONFIG = {
    "micro_batch_size": 4,     # 可以增加，0.5B模型显存需求低
    "global_batch_size": 64,   # 利用49GB显存优势
    "sequence_length": 2048,   # 适中长度，够用
    "max_steps": 500,          # 0.5B模型收敛快，减少steps
    "log_every_n_steps": 5     # 更频繁记录，便于监控
}

PEFT_CONFIG = {
    "peft_scheme": "lora",
    "micro_batch_size": 4,     # 利用显存充足优势
    "global_batch_size": 64,   # 增加batch size
    "sequence_length": 2048,   # 不需要4096，2048够用
    "max_steps": 200,          # 0.5B模型PEFT收敛很快
    "val_check_interval": 5,   # 更频繁验证
    # 关键PEFT配置（官方要求）
    "ckpt_async_save": False,
    "context_parallel_size": 1,
    "ddp": "megatron"
}

# 执行方式（必须使用）
if __name__ == "__main__":
    # 继续学习
    continual_recipe = setup_continual_pretraining()
    run.run(continual_recipe, executor=run.LocalExecutor())
    
    # PEFT微调
    peft_recipe = setup_peft_training()
    run.run(peft_recipe, executor=run.LocalExecutor())
```

### 11.3 评估指标
- 日语生成质量
- 推理速度
- 内存使用情况
- 学习效率
- **WandB监控指标**: 实时loss、perplexity、GPU利用率

## 12. 额外考虑

### 12.1 许可证
- Qwen2.5: Apache 2.0
- Wikipedia: CC BY-SA
- NeMo: Apache 2.0
- WandB: 免费学术使用

### 12.2 训练数据（基于LLM-JP专业语料库）
- **LLM-JP Wikipedia v3**: 1-5GB（NeMo Curator处理后）
  - 来源：国立情报学研究所（NII）专业清洗数据
  - 格式：train_text_document.bin/.idx, validation_text_document.bin/.idx
  - 质量：专业去重、过滤、语言检测的高质量日语语料
- **数据分割**: 95%训练，5%验证（适合0.5B模型）
- **处理工具**: NeMo Curator + Uzushio（企业级但适配0.5B需求）

### 12.3 计算资源（演示导向）
- GPU利用率: 预计70-80%（训练时），< 30%（推理演示时）
- 训练时间: 持续学习6小时，LoRA微调2小时
- 演示运行: 模型加载< 30秒，推理响应< 2秒
- 磁盘空间: 约需100GB（开发），20GB（演示环境）
- **演示时间**: 你的部分10分钟，队友10分钟，总共20分钟

## 13. 成功指标（0.5B模型workshop版）

### 13.1 技术方法论指标（主要）
- ✅ NeMo 2.0 recipe系统成功运行
- ✅ 继续学习→PEFT完整链路打通  
- ✅ WandB监控集成工作正常
- ✅ 快速迭代优势成功展示（训练时间<3小时总计）
- ✅ 团队模型交付格式标准化

### 13.2 演示价值指标（核心）
- **你的10分钟演示**：
  - ✅ NeMo 2.0方法论清晰展示
  - ✅ 0.5B模型教学优势突出
  - ✅ 技术流程演示流畅无故障
  - ✅ 时间控制精准（10分钟±30秒）
- **整体20分钟演示**：
  - ✅ 技术衔接清晰：继续学习→PEFT→SFT→DPO
  - ✅ 团队协作展示：统一WandB监控
  - ✅ Workshop教学价值突出

### 13.3 现实期望管理（重要）
- ❌ **不追求**：与大模型相当的生成质量
- ❌ **不强调**：复杂的性能优化
- ❌ **不过度**：企业级部署复杂度
- ✅ **突出价值**：方法论教学+快速迭代+资源效率

## 14. NeMo 2.0执行检查清单 (基于官方文档验证的正确方式)

### ✅ 14.1 开发前必检项目
- [ ] **确认容器版本**: 使用NeMo 25.04或更新版本
- [ ] **安装nemo-run**: `pip install nemo-run`（必需）
- [ ] **验证导入**: `import nemo_run as run` 和 `from nemo.collections import llm` 成功
- [ ] **WandB团队配置**: 确认entity和project权限
- [ ] **模型访问权限**: 确认Qwen2.5-0.5B的HuggingFace访问权限
- [ ] **本地存储**: 预留5-10GB空间用于模型和checkpoint

### 14.2 每日开发检查 (基于官方文档验证模式)
- [ ] **代码结构**: 必须使用`if __name__ == "__main__":`包装（官方要求）
- [ ] **配置方式**: 使用`run.Config`和recipe系统（官方方式）
- [ ] **执行方式**: 通过`run.run(recipe, executor=run.LocalExecutor())`（官方标准）
- [ ] **WandB集成**: 通过`lightning.pytorch.loggers.WandbLogger`集成到trainer中（官方验证）
- [ ] **模型导入**: 使用`llm.import_ckpt()`一次性导入本地（官方方式）

### 14.3 训练流程验证 (按官方文档标准)
- [ ] **模型导入**: `llm.import_ckpt()` 成功导入0.5B模型
- [ ] **继续学习**: `llm.qwen25_500m.pretrain_recipe()` 正常运行（官方文档recipe列表验证命名）
- [ ] **PEFT配置**: `peft_scheme="lora"` + 官方要求的特殊配置应用
- [ ] **checkpoint保存**: 训练过程正常保存checkpoint
- [ ] **推理测试**: `api.generate()` 能够正常推理

### 14.4 关键配置验证 (官方文档要求)
- [ ] **PEFT特殊配置**（官方要求）: 
  - `recipe.trainer.strategy.ckpt_async_save = False`
  - `recipe.trainer.strategy.context_parallel_size = 1`
  - `recipe.trainer.strategy.ddp = "megatron"`
- [ ] **数据配置**: `run.Config(DataModule, ...)` 正确配置（官方方式）
- [ ] **WandB Logger**: 正确集成到`recipe.trainer.logger`（官方验证）
- [ ] **checkpoint路径**: 路径格式符合NeMo标准

### 14.5 演示环境验证
- [ ] **推理配置**: MegatronStrategy + nl.Trainer 设置正确
- [ ] **checkpoint路径**: PEFT模型checkpoint存在且可访问
- [ ] **推理参数**: CommonInferenceParams 配置合理
- [ ] **WandB演示**: 实时监控正常工作
- [ ] **日语测试**: 日语输入输出正常显示

### 14.6 避免的常见错误 (基于官方文档分析)
- ❌ **不使用recipe系统**: 必须使用`llm.qwen25_500m.finetune_recipe()`（官方文档recipe列表验证）
- ❌ **忘记if __name__**: 这是官方要求的必需结构
- ❌ **直接运行.py**: 必须通过`run.run()`执行（官方标准）
- ❌ **错误的checkpoint路径**: 必须使用完整的NeMo checkpoint路径
- ❌ **忽略PEFT特殊配置**: LoRA训练必需的策略配置（官方要求）

## 15. 后续发展

### 15.1 扩展可能性
- 支持更大规模模型（1B、3B）
- 集成更多日语任务数据集
- 开发图形化界面工具
- **WandB模板**: 可复用的监控模板

### 15.2 社区贡献
- 开源完整实现代码
- 发布技术博客和教程
- 参与NeMo社区讨论
- **WandB案例**: 分享最佳实践

### 15.3 学术应用
- 性能基准测试报告
- 多语言模型比较研究
- 教育应用案例分析 
- **监控方法论**: WandB在NLP中的应用研究 