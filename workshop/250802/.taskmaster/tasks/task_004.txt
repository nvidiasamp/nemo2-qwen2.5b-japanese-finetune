# Task ID: 4
# Title: 使用NeMo Curator进行数据去重与格式化
# Status: pending
# Dependencies: 3
# Priority: high
# Description: 使用NeMo Curator对Uzushio处理过的数据进行高质量的去重和过滤，利用GPU加速（cuDF）提升处理效率。
# Details:
创建脚本 `scripts/02b_process_data_curator.py`。
1. 读取上一步生成的 `jsonl` 文件。
2. 应用 `WordCountFilter` 和 `LanguageIdentificationFilter`。
3. 应用 `ExactDuplicates` 和 `FuzzyDuplicates` 去重。
4. 使用cuDF后端加速处理，并将结果保存为Parquet格式。
```python
# scripts/02b_process_data_curator.py
from nemo_curator.main import main as curator_main
from nemo_curator.datasets import DocumentDataset
# ... (导入过滤器和去重模块) ...

input_dir = "./data/llm_jp_wiki/processed"
output_dir = "./data/llm_jp_wiki/nemo_format"

dataset = DocumentDataset.read_json(input_dir, backend="cudf")
# ... (应用过滤器和去重模块) ...
dataset.to_parquet(output_dir)
```

# Test Strategy:
脚本执行成功，在 `./data/llm_jp_wiki/nemo_format` 目录下生成Parquet文件。抽样检查数据质量，确认重复和低质量内容已被移除。

# Subtasks:
## 1. 配置并加载Uzushio输出数据 [pending]
### Dependencies: None
### Description: 根据需求配置数据加载参数，并将Uzushio输出的数据导入处理环境，确保数据完整性和可访问性。
### Details:
包括指定数据路径、格式、分片方式等，支持GPU加速的数据加载方案。

## 2. 应用过滤器 [pending]
### Dependencies: 4.1
### Description: 对加载的数据应用预设过滤规则，剔除不符合要求的数据行，为后续去重做准备。
### Details:
可包括长度、特殊字符、内容质量等多种过滤条件，确保数据质量。

## 3. 执行去重（精确与模糊） [pending]
### Dependencies: 4.2
### Description: 对过滤后的数据进行精确去重和模糊去重，消除重复内容，提升数据唯一性。
### Details:
采用哈希、相似度算法等多种策略，支持GPU加速处理大规模数据。

## 4. 保存并验证Parquet格式输出 [pending]
### Dependencies: 4.3
### Description: 将去重后的数据保存为Parquet格式，并进行格式和内容验证，确保输出文件可用。
### Details:
包括Parquet文件写入、数据一致性校验、文件完整性检查等步骤。

