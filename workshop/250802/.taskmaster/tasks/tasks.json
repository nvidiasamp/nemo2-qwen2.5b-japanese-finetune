{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "环境设置与项目初始化",
        "description": "验证NVIDIA NeMo 25.04 Docker容器环境，安装nemo-run等必要依赖，并初始化WandB项目用于团队协作和实验监控。",
        "details": "1. 启动NeMo容器: `docker run --gpus all -it --rm -v .:/workspace nvcr.io/nvidia/nemo:25.04`\n2. 在容器内安装依赖: `pip install nemo-run wandb`\n3. 登录WandB: `wandb login [YOUR_API_KEY]`\n4. 初始化WandB项目: 在WandB界面创建名为 `qwen25-japanese-continual` 和 `qwen25-japanese-peft` 的项目，并设置团队实体为 `your-team`。\n5. 创建项目目录结构:\n/workspace\n  - /data\n  - /models\n  - /scripts\n  - /logs",
        "testStrategy": "成功启动容器，`import nemo_run` 和 `import wandb` 不报错。WandB网站上能看到新建的项目。",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Qwen2.5-0.5B模型本地导入与转换",
        "description": "使用NeMo 2.0的llm.import_ckpt工具，从HuggingFace Hub下载Qwen2.5-0.5B模型并转换为NeMo兼容的.nemo格式，以便后续训练使用。",
        "details": "创建一个Python脚本 `scripts/01_import_model.py`。该脚本应只执行一次，将模型保存在本地 `./models/qwen25_0.5b` 目录。\n```python\n# scripts/01_import_model.py\nfrom nemo.collections import llm\n\n# 官方文档验证的正确导入方式\nif __name__ == \"__main__\":  # 官方要求必需\n    llm.import_ckpt(\n        model=llm.Qwen2Model(llm.Qwen25Config500M()),  # 官方文档配置\n        source='hf://Qwen/Qwen2.5-0.5B',\n        output_path='./models/qwen25_0.5b.nemo',\n        overwrite=False\n    )\n    print('模型导入完成！')\n```",
        "testStrategy": "执行脚本后，在 `./models/` 目录下成功生成 `qwen25_0.5b.nemo` 文件，大小约为1-2GB。检查日志无报错。",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "LLM-JP日语语料库下载与Uzushio预处理",
        "description": "从LLM-JP GitLab仓库下载日语Wikipedia v3语料库，并使用Uzushio工具进行初步的清洗和过滤，为0.5B模型准备适量的高质量数据。",
        "details": "创建一个脚本 `scripts/02a_process_data_uzushio.py`。\n1. 使用 `git sparse-checkout` 只下载 `ja/ja_wiki` 部分以节省空间和时间。\n2. 调用Uzushio对下载的原始数据进行处理。\n```python\n# scripts/02a_process_data_uzushio.py\nimport subprocess\nfrom pathlib import Path\nfrom uzushio.main import main as uzushio_main\n\ndata_dir = Path(\"./data/llm_jp_wiki\")\n# ... (git clone 和 sparse-checkout 代码) ...\n\nuzushio_config = {\n    \"input_dir\": str(data_dir / \"ja/ja_wiki\"),\n    \"output_dir\": str(data_dir / \"processed\"),\n    \"max_docs\": 100000,  # 限制文档数量，适合0.5B模型\n    \"min_text_length\": 100,\n    \"language\": \"ja\",\n    \"num_workers\": 4\n}\nuzushio_main(**uzushio_config)\n```",
        "testStrategy": "脚本执行成功，在 `./data/llm_jp_wiki/processed` 目录下生成处理后的 `jsonl` 文件。检查文件内容是否为清洗后的日语文本。",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "配置并执行sparse-checkout下载",
            "description": "使用Git的sparse-checkout功能，仅下载所需的数据子目录或文件，避免拉取整个大型仓库。",
            "dependencies": [],
            "details": "包括初始化本地仓库、配置sparse-checkout模式、编辑.git/info/sparse-checkout文件指定需要的路径，并执行拉取操作。",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "安装并配置Uzushio",
            "description": "在本地环境中安装Uzushio工具，并完成相关依赖配置，确保其可用于后续数据处理。",
            "dependencies": [
              1
            ],
            "details": "根据Uzushio官方文档或项目说明，完成软件包安装、环境变量设置及依赖库配置。",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "编写数据处理脚本",
            "description": "开发脚本对下载的数据进行预处理，包括格式转换、数据清洗等操作，以满足后续分析需求。",
            "dependencies": [
              2
            ],
            "details": "根据数据特点和预处理需求，选择合适的编程语言和库，编写并测试数据处理脚本。",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "验证清洗后数据质量",
            "description": "对经过预处理的数据进行质量检查，确保数据完整性、准确性和可用性。",
            "dependencies": [
              3
            ],
            "details": "设计并执行数据质量验证流程，包括缺失值检测、格式一致性检查及样本抽查等。",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 4,
        "title": "使用NeMo Curator进行数据去重与格式化",
        "description": "使用NeMo Curator对Uzushio处理过的数据进行高质量的去重和过滤，利用GPU加速（cuDF）提升处理效率。",
        "details": "创建脚本 `scripts/02b_process_data_curator.py`。\n1. 读取上一步生成的 `jsonl` 文件。\n2. 应用 `WordCountFilter` 和 `LanguageIdentificationFilter`。\n3. 应用 `ExactDuplicates` 和 `FuzzyDuplicates` 去重。\n4. 使用cuDF后端加速处理，并将结果保存为Parquet格式。\n```python\n# scripts/02b_process_data_curator.py\nfrom nemo_curator.main import main as curator_main\nfrom nemo_curator.datasets import DocumentDataset\n# ... (导入过滤器和去重模块) ...\n\ninput_dir = \"./data/llm_jp_wiki/processed\"\noutput_dir = \"./data/llm_jp_wiki/nemo_format\"\n\ndataset = DocumentDataset.read_json(input_dir, backend=\"cudf\")\n# ... (应用过滤器和去重模块) ...\ndataset.to_parquet(output_dir)\n```",
        "testStrategy": "脚本执行成功，在 `./data/llm_jp_wiki/nemo_format` 目录下生成Parquet文件。抽样检查数据质量，确认重复和低质量内容已被移除。",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "配置并加载Uzushio输出数据",
            "description": "根据需求配置数据加载参数，并将Uzushio输出的数据导入处理环境，确保数据完整性和可访问性。",
            "dependencies": [],
            "details": "包括指定数据路径、格式、分片方式等，支持GPU加速的数据加载方案。",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "应用过滤器",
            "description": "对加载的数据应用预设过滤规则，剔除不符合要求的数据行，为后续去重做准备。",
            "dependencies": [
              1
            ],
            "details": "可包括长度、特殊字符、内容质量等多种过滤条件，确保数据质量。",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "执行去重（精确与模糊）",
            "description": "对过滤后的数据进行精确去重和模糊去重，消除重复内容，提升数据唯一性。",
            "dependencies": [
              2
            ],
            "details": "采用哈希、相似度算法等多种策略，支持GPU加速处理大规模数据。",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "保存并验证Parquet格式输出",
            "description": "将去重后的数据保存为Parquet格式，并进行格式和内容验证，确保输出文件可用。",
            "dependencies": [
              3
            ],
            "details": "包括Parquet文件写入、数据一致性校验、文件完整性检查等步骤。",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 5,
        "title": "数据二值化以适配NeMo训练",
        "description": "将NeMo Curator处理后的Parquet数据转换为NeMo框架训练所需的标准二进制格式（.bin和.idx文件）。",
        "details": "创建脚本 `scripts/02c_binarize_data.py`。\n使用 `nemo.collections.nlp.data.language_modeling.preprocess_data_for_megatron` 模块。\n需要Qwen2.5的tokenizer配置文件路径，该文件应在模型导入（任务2）后可用。\n```python\n# scripts/02c_binarize_data.py\nfrom nemo.collections.nlp.data.language_modeling.preprocess_data_for_megatron import main as preprocess_main\n\n# 假设tokenizer文件在导入的模型目录中\ntokenizer_path = './models/qwen25_0.5b.nemo' # NeMo 2.0可以直接引用.nemo文件作为tokenizer\n\npreprocess_args = [\n    '--input', './data/llm_jp_wiki/nemo_format',\n    '--output-prefix', './data/llm_jp_wiki/nemo_binary/ja_wiki',\n    '--tokenizer-model', tokenizer_path,\n    '--dataset-impl', 'mmap',\n    '--tokenizer-type', 'HuggingFaceTokenizer',\n    '--workers', '4',\n    '--split-sentences'\n]\n# 使用subprocess或直接调用main\n# preprocess_main(preprocess_args)\n```",
        "testStrategy": "脚本执行成功，在 `./data/llm_jp_wiki/nemo_binary/` 目录下生成 `ja_wiki_text_document.bin` 和 `ja_wiki_text_document.idx` 等文件。",
        "priority": "high",
        "dependencies": [
          2,
          4
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "配置tokenizer与输入输出路径",
            "description": "根据项目需求，正确配置所需的tokenizer，并设置数据二值化脚本的输入和输出文件路径，确保后续流程能够顺利读取和保存数据。",
            "dependencies": [],
            "details": "包括选择合适的tokenizer模型、安装相关依赖、检查tokenizer参数设置，以及明确原始数据文件和目标二值化文件（如.bin和.idx）的存放路径。",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "编写并运行二值化脚本",
            "description": "根据配置，编写实现数据二值化的脚本，并在本地或服务器环境中运行，生成二值化后的数据文件。",
            "dependencies": [
              1
            ],
            "details": "脚本需实现数据读取、tokenizer处理、二值化转换及输出保存，运行过程中需关注日志输出，及时排查和修正可能出现的错误。",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "验证生成的.bin和.idx文件",
            "description": "对生成的二值化数据文件（.bin和.idx）进行完整性和可用性验证，确保文件内容符合预期标准。",
            "dependencies": [
              2
            ],
            "details": "包括文件大小、内容格式、数据一致性检查，可通过脚本读取部分内容或配套工具进行解析，确保后续流程能够正确使用这些文件。",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 6,
        "title": "构建继续学习训练脚本",
        "description": "基于NeMo 2.0的recipe系统，编写日语继续学习的训练脚本。该脚本使用`llm.qwen25_500m.pretrain_recipe`，并配置处理好的日语数据集。",
        "details": "创建脚本 `scripts/03_run_continual_learning.py`。\n```python\n# scripts/03_run_continual_learning.py\nimport nemo_run as run\nfrom nemo.collections import llm\n\ndef setup_continual_training():\n    recipe = llm.qwen25_500m.pretrain_recipe(\n        dir=\"./models/checkpoints/qwen25_continual\",\n        name=\"japanese_continual_learning\"\n    )\n    recipe.model.restore_from_path = './models/qwen25_0.5b.nemo' # 从导入的模型开始\n    recipe.data = run.Config(\n        llm.PreTrainingDataModule,\n        paths=['./data/llm_jp_wiki/nemo_binary/ja_wiki_text_document'],\n        seq_length=2048,\n        micro_batch_size=4,\n        global_batch_size=64\n    )\n    recipe.trainer.max_steps = 1000\n    return recipe\n\nif __name__ == \"__main__\":\n    recipe = setup_continual_training()\n    # run.run(recipe, executor=run.LocalExecutor()) # 暂时注释，待WandB集成后执行\n```",
        "testStrategy": "脚本可以无报错地初始化recipe对象。配置的数据路径、模型路径和超参数正确无误。",
        "priority": "high",
        "dependencies": [
          2,
          5
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "集成WandB到继续学习流程",
        "description": "将WandB监控功能集成到继续学习训练脚本中，以便实时跟踪损失、学习率和GPU利用率等关键指标。",
        "details": "修改 `scripts/03_run_continual_learning.py`。\n```python\n# ... imports ...\nfrom lightning.pytorch.loggers import WandbLogger\n\ndef setup_continual_training_with_wandb():\n    # ... (recipe 定义) ...\n    wandb_logger = WandbLogger(\n        project=\"qwen25-japanese-continual\",\n        name=\"continual-learning-phase\",\n        entity=\"your-team\",\n        tags=[\"continual-learning\", \"qwen2.5-0.5b\"]\n    )\n    recipe.trainer.logger = wandb_logger\n    recipe.trainer.log_every_n_steps = 10\n    return recipe\n\nif __name__ == \"__main__\":\n    recipe = setup_continual_training_with_wandb()\n    run.run(recipe, executor=run.LocalExecutor())\n```",
        "testStrategy": "运行训练脚本后，在WandB对应的项目页面能看到一个新的run，并且loss等指标被实时记录。",
        "priority": "medium",
        "dependencies": [
          6
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "执行并验证日语继续学习训练",
        "description": "执行完整的日语继续学习训练流程，监控训练过程，并在训练完成后保存模型checkpoint，为PEFT阶段做准备。",
        "details": "在终端中执行 `python scripts/03_run_continual_learning.py`。预计训练时间为1-2小时。密切关注WandB上的loss曲线，确保其平稳下降。训练完成后，checkpoint将保存在 `./models/checkpoints/qwen25_continual` 目录。",
        "testStrategy": "训练成功完成，没有中途报错。WandB上的loss曲线显示模型已收敛。在指定的目录中找到了.nemo格式的checkpoint文件。",
        "priority": "high",
        "dependencies": [
          7
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "构建PEFT-LoRA微调脚本",
        "description": "基于NeMo 2.0的recipe系统，编写PEFT-LoRA微调脚本。脚本使用`llm.qwen25_500m.finetune_recipe`，并加载上一阶段继续学习产出的模型checkpoint。",
        "details": "创建脚本 `scripts/04_run_peft_tuning.py`。\n```python\n# scripts/04_run_peft_tuning.py\nimport nemo_run as run\nfrom nemo.collections import llm\nfrom nemo import lightning as nl\n\ndef setup_peft_training():\n    recipe = llm.qwen25_500m.finetune_recipe(\n        dir=\"./models/checkpoints/qwen25_peft\",\n        name=\"japanese_lora\",\n        peft_scheme=\"lora\"\n    )\n    # 恢复继续学习后的模型\n    recipe.model.restore_from_path = './models/checkpoints/qwen25_continual/checkpoints/last.nemo' # 指向正确的checkpoint\n    # 官方要求的PEFT特殊配置\n    recipe.trainer.strategy.ckpt_async_save = False\n    recipe.trainer.strategy.context_parallel_size = 1\n    recipe.trainer.strategy.ddp = \"megatron\"\n    recipe.trainer.max_steps = 500\n    # ... (配置微调数据) ...\n    return recipe\n```",
        "testStrategy": "脚本可以无报错地初始化PEFT recipe对象。所有关键配置（peft_scheme, ckpt_async_save等）均已正确设置。",
        "priority": "high",
        "dependencies": [
          8
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "集成WandB到PEFT微调流程",
        "description": "将WandB监控功能集成到PEFT-LoRA微调脚本中，使用独立的WandB项目或命名空间来区分不同阶段的实验。",
        "details": "修改 `scripts/04_run_peft_tuning.py`。\n```python\n# ... imports ...\nfrom lightning.pytorch.loggers import WandbLogger\n\ndef setup_peft_training_with_wandb():\n    # ... (recipe 定义) ...\n    wandb_logger = WandbLogger(\n        project=\"qwen25-japanese-peft\",\n        name=\"lora-finetuning-phase\",\n        entity=\"your-team\",\n        tags=[\"peft\", \"lora\", \"qwen2.5-0.5b\"]\n    )\n    recipe.trainer.logger = wandb_logger\n    recipe.trainer.log_every_n_steps = 5\n    return recipe\n\nif __name__ == \"__main__\":\n    recipe = setup_peft_training_with_wandb()\n    run.run(recipe, executor=run.LocalExecutor())\n```",
        "testStrategy": "运行微调脚本后，在WandB的`qwen25-japanese-peft`项目页面能看到新的run，并且相关指标被实时记录。",
        "priority": "medium",
        "dependencies": [
          9
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 11,
        "title": "执行并验证PEFT-LoRA微调",
        "description": "执行PEFT-LoRA微调训练，监控过程，并保存最终的LoRA适配器权重。这是获取最终可演示模型的核心步骤。",
        "details": "在终端中执行 `python scripts/04_run_peft_tuning.py`。预计训练时间为30-60分钟。检查WandB上的验证损失（val_loss）以判断收敛情况。训练完成后，适配器权重将保存在 `./models/checkpoints/qwen25_peft` 目录。",
        "testStrategy": "训练成功完成。WandB图表显示模型收敛。在指定目录中找到了包含LoRA权重的.nemo checkpoint文件。",
        "priority": "high",
        "dependencies": [
          10
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 12,
        "title": "开发模型推理与评估脚本",
        "description": "开发一个推理脚本，用于加载经过LoRA微调的模型，并对给定的日语提示词生成文本。该脚本将作为最终演示的基础。",
        "details": "创建脚本 `scripts/05_run_inference.py`。\n```python\n# scripts/05_run_inference.py\nimport torch\nimport nemo.lightning as nl\nfrom nemo.collections.llm import api\n\ndef setup_inference_trainer():\n    strategy = nl.MegatronStrategy(context_parallel_size=1)\n    return nl.Trainer(accelerator=\"gpu\", devices=1, strategy=strategy)\n\nif __name__ == \"__main__\":\n    trainer = setup_inference_trainer()\n    peft_model_path = './models/checkpoints/qwen25_peft/checkpoints/last.nemo'\n    prompts = [\"日本の美しい季節について教えてください。\", \"人工知能の未来はどうなると思いますか？\"]\n    \n    results = api.generate(\n        path=peft_model_path,\n        prompts=prompts,\n        trainer=trainer,\n        text_only=True\n    )\n    print(results)\n```",
        "testStrategy": "脚本成功加载模型并对给定的日语提示词生成了连贯的文本。输出结果符合预期。",
        "priority": "high",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "配置推理环境与依赖",
            "description": "搭建并配置所需的推理环境，安装相关依赖库和工具，确保脚本开发和模型推理能够顺利进行。",
            "dependencies": [],
            "details": "包括Python环境、深度学习框架（如PyTorch、TensorFlow）、推理相关库（如ONNXRuntime）等的安装与配置。",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "编写加载与推理代码",
            "description": "开发脚本，实现模型的加载、输入数据的预处理、推理执行及输出的后处理。",
            "dependencies": [
              1
            ],
            "details": "根据模型类型编写加载模型、数据预处理、推理执行和结果后处理的代码，确保推理流程完整且高效。",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "设计评估用例",
            "description": "制定用于评估推理脚本准确性和鲁棒性的测试用例，覆盖常见和边界场景。",
            "dependencies": [
              2
            ],
            "details": "包括输入样本的准备、预期输出的设定，以及评估指标的确定，确保评估全面。",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "验证输出文本质量",
            "description": "对推理脚本输出的文本进行质量验证，评估其准确性、相关性和可读性。",
            "dependencies": [
              3
            ],
            "details": "采用人工或自动化方法对输出文本进行分析，依据预设标准判断输出质量，必要时进行脚本优化。",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 13,
        "title": "集成WandB到推理演示流程",
        "description": "将WandB监控功能集成到推理脚本中，用于在现场演示时实时记录和展示每个推理请求的耗时、输入和输出，增强演示效果。",
        "details": "修改 `scripts/05_run_inference.py`，添加WandB的初始化和日志记录功能。\n```python\n# ... imports ...\nimport wandb\nimport time\n\n# ...\nif __name__ == \"__main__\":\n    wandb.init(project=\"qwen25-demo-live\", entity=\"your-team\")\n    # ... (trainer, model_path, prompts 定义) ...\n    start_time = time.time()\n    results = api.generate(...)\n    inference_time = time.time() - start_time\n    \n    wandb.log({\n        \"prompt\": prompts[0],\n        \"response\": results[0],\n        \"inference_time_sec\": inference_time\n    })\n    print(results)\n    wandb.finish()\n```",
        "testStrategy": "运行推理脚本后，WandB的`qwen25-demo-live`项目页面出现新的run，并记录了prompt、response和推理时间等信息。",
        "priority": "medium",
        "dependencies": [
          12
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "创建模型交付包与文档",
        "description": "将最终经过LoRA微调的模型打包成标准格式，并编写一份清晰的交付文档（Model Card），说明模型来源、训练过程和使用方法，以便交付给SFT/DPO团队。",
        "details": "1. 确定交付格式：提供最终的.nemo checkpoint路径。\n2. 编写 `MODEL_CARD.md`，内容包括：\n   - 基础模型: Qwen2.5-0.5B\n   - 训练阶段1: 日语Wikipedia继续学习 (checkpoint: ...)\n   - 训练阶段2: LoRA微调 (adapter checkpoint: ...)\n   - 使用方法: 提供加载和运行推理脚本的命令。\n   - WandB链接: 提供相关实验的链接。\n3. 将checkpoint和文档打包成一个zip文件。",
        "testStrategy": "SFT/DPO团队成员能够根据交付物和文档，成功加载模型并复现推理结果。",
        "priority": "high",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "准备并排练10分钟技术演示",
        "description": "根据PRD要求，准备并反复排练10分钟的技术演示。重点突出NeMo 2.0的方法论、0.5B模型的快速迭代优势和WandB的协作价值，而非生成效果本身。",
        "details": "1. **演示结构 (10分钟)**:\n   - NeMo 2.0技术栈 (4分钟): 展示recipe脚本，解释`run.run`。\n   - 快速迭代展示 (4分钟): 展示WandB面板，对比继续学习和PEFT的训练时长和loss曲线。\n   - 教学价值与团队协作 (2分钟): 展示模型交付物，说明与SFT/DPO的接口。\n2. **演示材料**: 准备简洁的PPT，主要内容以实时代码和WandB仪表盘展示为主。\n3. **排练**: 多次计时排练，确保内容能在10分钟内清晰讲完。",
        "testStrategy": "能够流畅、自信地在10分钟内完成所有演示环节。演示内容清晰地传达了项目的核心技术价值和教学意义。与队友的20分钟整体演示衔接顺畅。",
        "priority": "high",
        "dependencies": [
          12,
          13,
          14
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "梳理演示结构与内容",
            "description": "明确演示的目标受众和目的，梳理演示的整体结构，确定关键内容和逻辑顺序，列出演示要点。",
            "dependencies": [],
            "details": "包括思考演讲目标、观众需求，列出3-5个核心要点，规划内容逻辑。",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "准备PPT与演示材料",
            "description": "根据梳理好的结构和内容，制作PPT和其他辅助演示材料，确保视觉表达清晰、重点突出。",
            "dependencies": [
              1
            ],
            "details": "制作幻灯片、图表、视频或实物模型，注意PPT简洁明了，避免信息过载。",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "多次计时排练",
            "description": "对照演示材料进行多次计时排练，熟悉内容，优化表达，确保时间控制合理。",
            "dependencies": [
              2
            ],
            "details": "每次排练后记录用时，调整内容节奏，重点练习薄弱环节。",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "演示内容与队友衔接演练",
            "description": "与队友共同演练，确保各自内容衔接流畅，分工明确，提升团队配合度。",
            "dependencies": [
              3
            ],
            "details": "模拟正式演示流程，重点关注交接环节和互动配合，及时沟通调整。",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-07-09T10:33:07.997Z",
      "updated": "2025-07-09T10:33:07.997Z",
      "description": "Tasks for master context"
    }
  }
}