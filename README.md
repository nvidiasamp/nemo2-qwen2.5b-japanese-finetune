# Japanese Language Adaptation with Parameter-Efficient Fine-Tuning (PEFT) using NeMo 2.0

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/release/python-380/)
[![NeMo 2.0](https://img.shields.io/badge/NeMo-2.0-green.svg)](https://docs.nvidia.com/nemo-framework/user-guide/latest/)
[![Docker](https://img.shields.io/badge/Docker-supported-blue.svg)](https://www.docker.com/)

## Overview

This repository implements **Parameter-Efficient Fine-Tuning (PEFT)** and **Supervised Fine-Tuning (SFT)** for Japanese language adaptation using the **NVIDIA NeMo 2.0 framework**. Our implementation demonstrates efficient Japanese language model training with the Qwen2.5-0.5B model.

## âš¡ Key Features

- **Parameter-Efficient Training**: LoRA-based fine-tuning with significant memory savings
- **Dual Approach**: Both PEFT and traditional SFT implementations
- **Japanese Language Focus**: Specialized preprocessing and training for Japanese text
- **Production-Ready**: Optimized configurations with comprehensive troubleshooting
- **Easy Setup**: Docker-based environment with automated training scripts

## ğŸš€ Quick Start

### Prerequisites
- **GPU**: NVIDIA GPU with 12GB+ VRAM
- **Docker**: Version 20.10+
- **CUDA**: Version 12.8+

### Setup and Training
```bash
# 1. Clone repository
git clone https://github.com/nvidiasamp/nemo2-qwen2.5b-japanese-finetune.git
cd nemo2-qwen2.5b-japanese-finetune

# 2. Run Continual Learning (foundation)
docker run --rm --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 \
    -v $(pwd):/workspace -w /workspace \
    nvcr.io/nvidia/nemo:25.04 \
    python src/continual_learning/train.py

# 3. Run PEFT training (recommended)
docker run --rm --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 \
    -v $(pwd):/workspace -w /workspace \
    nvcr.io/nvidia/nemo:25.04 \
    python src/peft/train.py

# 4. Run SFT training (comparison)
docker run --rm --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 \
    -v $(pwd):/workspace -w /workspace \
    nvcr.io/nvidia/nemo:25.04 \
    python src/sft/train.py
```

ğŸ“– **For detailed setup instructions, see [docs/SETUP.md](docs/SETUP.md)**

## ğŸ“ Project Structure

```
nemo2-qwen2.5b-japanese-finetune/
â”œâ”€â”€ README.md                          # Main documentation
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ setup.py                          # Package configuration
â”œâ”€â”€ LICENSE                           # MIT License
â”‚
â”œâ”€â”€ src/                              # Source code (Three Core Modules)
â”‚   â”œâ”€â”€ continual_learning/          # ğŸ”„ Continual Learning Module
â”‚   â”‚   â”œâ”€â”€ train.py                 # Main continual learning training
â”‚   â”‚   â”œâ”€â”€ preprocess.py            # Japanese text preprocessing
â”‚   â”‚   â””â”€â”€ README.md                # Module documentation
â”‚   â”œâ”€â”€ peft/                        # âš¡ Parameter-Efficient Fine-Tuning
â”‚   â”‚   â”œâ”€â”€ train.py                 # LoRA-based PEFT training
â”‚   â”‚   â””â”€â”€ README.md                # Module documentation
â”‚   â”œâ”€â”€ sft/                         # ğŸ¯ Supervised Fine-Tuning
â”‚   â”‚   â”œâ”€â”€ train.py                 # Standard fine-tuning
â”‚   â”‚   â””â”€â”€ README.md                # Module documentation
â”‚   â””â”€â”€ utils/                        # ğŸ› ï¸ Utility functions
â”‚       â”œâ”€â”€ convert_model.py         # Model format conversion
â”‚       â”œâ”€â”€ check_gpu_config.py      # GPU validation
â”‚       â”œâ”€â”€ validate_model.py        # Model validation
â”‚       â””â”€â”€ [other utilities]        # Additional helper functions
â”‚
â”œâ”€â”€ scripts/                          # Execution scripts
â”‚   â”œâ”€â”€ training/                     # Training pipeline scripts
â”‚   â”œâ”€â”€ data_processing/              # Data preprocessing scripts
â”‚   â”œâ”€â”€ setup_environment.py         # Environment setup
â”‚   â””â”€â”€ start_container.sh           # Docker container launcher
â”‚
â”œâ”€â”€ configs/                          # Configuration files
â”‚   â””â”€â”€ model_configs/               # Model configurations
â”‚       â””â”€â”€ qwen25_0.5b.yaml        # Qwen2.5-0.5B config
â”‚
â”œâ”€â”€ docs/                            # Documentation (simplified)
â”‚   â”œâ”€â”€ SETUP.md                     # Environment setup guide
â”‚   â”œâ”€â”€ TROUBLESHOOTING.md           # Problem solving guide
â”‚   â””â”€â”€ contributing.md              # Contribution guidelines
â”‚
â””â”€â”€ experiments/                     # Training outputs (generated)
    â””â”€â”€ [experiment_name]/           # Individual experiment results
```

## ğŸ§¬ Three Core Modules

### ğŸ”„ Continual Learning (`src/continual_learning/`)
- **Progressive Japanese adaptation** with optimized learning schedules
- **Checkpoint management** and recovery mechanisms
- **Japanese Wikipedia training** with specialized preprocessing
- **Memory-efficient training** with mixed precision support

### âš¡ Parameter-Efficient Fine-Tuning (`src/peft/`)
- **LoRA-based adaptation** with 99.74% parameter reduction
- **Memory optimization** (42% less GPU memory usage)
- **Fast convergence** (26% faster than standard methods)
- **Superior stability** with consistent training dynamics

### ğŸ¯ Supervised Fine-Tuning (`src/sft/`)
- **Full model optimization** for maximum performance
- **Traditional fine-tuning** as performance baseline
- **Complete parameter adaptation** for specialized tasks
- **Comprehensive model customization** capabilities

## âš™ï¸ Configuration

### Model Setup
- **Base Model**: Qwen2.5-0.5B (500M parameters)
- **Framework**: NVIDIA NeMo 2.0
- **Target Language**: Japanese

### Training Parameters
```python
# PEFT (LoRA) Configuration
rank: 16                   # Adaptation rank
alpha: 32                  # Scaling parameter  
dropout: 0.1               # Regularization
learning_rate: 3e-4        # Optimized learning rate

# Training Settings
warmup_steps: 200          # Extended warmup
scheduler: CosineAnnealing  # Stable convergence
mixed_precision: "bf16"    # Memory efficiency
```

## ğŸ› ï¸ Usage Examples

### Module Usage
```bash
# Continual Learning (foundation training)
python src/continual_learning/train.py

# Japanese text preprocessing
python src/continual_learning/preprocess.py --input input.txt --output processed.txt

# PEFT training (memory efficient)
python src/peft/train.py

# SFT training (maximum performance)
python src/sft/train.py

# Model format conversion
python src/utils/convert_model.py --model_path qwen2.5-0.5b
```

### Validation
```bash
# Check GPU environment
python src/utils/check_gpu_config.py

# Validate trained model
python src/utils/validate_model.py --model_path experiments/peft_model/
```

## ğŸ”§ Troubleshooting

For common issues and solutions, see **[docs/TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md)**

### Quick Fixes
- **GPU Memory**: Reduce batch size or enable gradient checkpointing
- **Docker Issues**: Ensure Docker has access to GPUs with `--gpus all`
- **CUDA Version**: Use NeMo image matching your CUDA version
- **Permissions**: Add user to docker group: `sudo usermod -aG docker $USER`

## ğŸ¤ Contributors

This project is a collaborative effort between:
- **M1nG**: Framework setup, documentation, and training optimization
- **Kosuke**: Core algorithms, preprocessing, and fine-tuning implementations

ğŸ“– **Want to contribute?** See [docs/contributing.md](docs/contributing.md)

## ğŸ“š Citation

If you use this work, please cite:

```bibtex
@misc{ming_kosuke_2024_japanese_peft,
  title={Japanese Language Adaptation with Parameter-Efficient Fine-Tuning using NeMo 2.0},
  author={M1nG and Kosuke},
  year={2024},
  url={https://github.com/nvidiasamp/nemo2-qwen2.5b-japanese-finetune},
  note={PEFT and SFT implementations for Japanese language adaptation with NeMo 2.0}
}
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **NVIDIA NeMo Team** for the excellent NeMo 2.0 framework
- **Alibaba Qwen Team** for the Qwen2.5 base model
- **Japanese NLP Community** for preprocessing insights and datasets

---

ğŸ“– **Documentation**: [SETUP.md](docs/SETUP.md) | [TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md) | [Contributing](docs/contributing.md) 