{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4cd061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import fiddle as fdl\n",
    "import lightning.pytorch as p\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from nemo import lightning as nl\n",
    "from nemo.collections import llm\n",
    "from nemo.lightning.pytorch.callbacks import JitConfig, JitTransform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df74bee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JapaneseWikiDataset(Dataset):\n",
    "    \"\"\"日本語WikipediaデータセットのためのDatasetクラス\"\"\"\n",
    "\n",
    "    def __init__(self, data_files: List[str], tokenizer, seq_length: int = 512,\n",
    "                 instruction_template: str = None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_length = seq_length\n",
    "        self.examples = []\n",
    "        self.instruction_template = instruction_template or self._default_instruction_template()\n",
    "\n",
    "        # データファイルの読み込み\n",
    "        for file_path in data_files:\n",
    "            self._load_file(file_path)\n",
    "\n",
    "        print(f\"Loaded {len(self.examples)} examples from {len(data_files)} files\")\n",
    "\n",
    "    def _default_instruction_template(self):\n",
    "        \"\"\"デフォルトのインストラクションテンプレート\"\"\"\n",
    "        return \"\"\"以下の文章を要約してください。\n",
    "\n",
    "文章: {text}\n",
    "\n",
    "要約:\"\"\"\n",
    "\n",
    "    def _load_file(self, file_path: str):\n",
    "        \"\"\"JSONLファイルの読み込み（gzip対応）\"\"\"\n",
    "        if file_path.endswith('.gz'):\n",
    "            with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    data = json.loads(line.strip())\n",
    "                    if 'text' in data and data['text'].strip():\n",
    "                        self.examples.append(data['text'])\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    data = json.loads(line.strip())\n",
    "                    if 'text' in data and data['text'].strip():\n",
    "                        self.examples.append(data['text'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.examples[idx]\n",
    "\n",
    "        # テキストを短く切って、要約タスクとして使用\n",
    "        # 実際のSFTでは、より適切な質問-回答ペアを使用することを推奨\n",
    "        if len(text) > 200:\n",
    "            input_text = text[:200]\n",
    "            target_text = text[:50]  # 簡単な要約として最初の50文字を使用\n",
    "\n",
    "            # インストラクション形式に変換\n",
    "            full_text = self.instruction_template.format(text=input_text) + \" \" + target_text\n",
    "        else:\n",
    "            full_text = text\n",
    "\n",
    "        # トークナイズ\n",
    "        encoded = self.tokenizer(\n",
    "            full_text,\n",
    "            max_length=self.seq_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'].squeeze(),\n",
    "            'attention_mask': encoded['attention_mask'].squeeze(),\n",
    "            'labels': encoded['input_ids'].squeeze()  # SFTでは入力と同じ\n",
    "        }\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"バッチのコレート関数\"\"\"\n",
    "        input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "        attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "        labels = torch.stack([item['labels'] for item in batch])\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f027c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JapaneseWikiDataModule(pl.LightningDataModule):\n",
    "    \"\"\"日本語WikipediaデータのためのLightningDataModule\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        data_dir: str = \"./data/ja_wiki\",\n",
    "        seq_length: int = 512,\n",
    "        micro_batch_size: int = 1,\n",
    "        global_batch_size: int = 2,\n",
    "        num_workers: int = 0,\n",
    "        pin_memory: bool = True,\n",
    "        persistent_workers: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.seq_length = seq_length\n",
    "        self.micro_batch_size = micro_batch_size\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.persistent_workers = persistent_workers\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "        \"\"\"データセットのセットアップ\"\"\"\n",
    "        # トレーニングデータファイルの取得\n",
    "        train_files = sorted(list(self.data_dir.glob(\"train_*.jsonl*\")))\n",
    "        val_files = sorted(list(self.data_dir.glob(\"validation_*.jsonl*\")))\n",
    "\n",
    "        if not train_files:\n",
    "            raise ValueError(f\"No training files found in {self.data_dir}\")\n",
    "\n",
    "        # データセットの作成\n",
    "        self.train_dataset = JapaneseWikiDataset(\n",
    "            [str(f) for f in train_files],\n",
    "            self.tokenizer,\n",
    "            self.seq_length\n",
    "        )\n",
    "\n",
    "        if val_files:\n",
    "            self.val_dataset = JapaneseWikiDataset(\n",
    "                [str(f) for f in val_files],\n",
    "                self.tokenizer,\n",
    "                self.seq_length\n",
    "            )\n",
    "        else:\n",
    "            # 検証データがない場合は、トレーニングデータの一部を使用\n",
    "            print(\"No validation files found, using 10% of training data for validation\")\n",
    "            train_size = int(0.9 * len(self.train_dataset))\n",
    "            val_size = len(self.train_dataset) - train_size\n",
    "            self.train_dataset, self.val_dataset = torch.utils.data.random_split(\n",
    "                self.train_dataset, [train_size, val_size]\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.micro_batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            persistent_workers=self.persistent_workers,\n",
    "            collate_fn=self.train_dataset.collate_fn if hasattr(self.train_dataset, 'collate_fn') else None,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.micro_batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            persistent_workers=self.persistent_workers,\n",
    "            collate_fn=self.val_dataset.collate_fn if hasattr(self.val_dataset, 'collate_fn') else None,\n",
    "            shuffle=False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "971d9885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def japanese_wiki_data(tokenizer, data_dir=\"./data/ja_wiki\", mbs=1, gbs=2) -> pl.LightningDataModule:\n",
    "    \"\"\"日本語Wikipediaデータモジュールをインスタンス化して返す\n",
    "\n",
    "    Args:\n",
    "        tokenizer (AutoTokenizer): 使用するトークナイザー\n",
    "        data_dir (str): ja_wikiデータのディレクトリパス\n",
    "        mbs (int): マイクロバッチサイズ\n",
    "        gbs (int): グローバルバッチサイズ\n",
    "\n",
    "    Returns:\n",
    "        pl.LightningDataModule: トレーニング用のデータセット\n",
    "    \"\"\"\n",
    "    return JapaneseWikiDataModule(\n",
    "        tokenizer=tokenizer,\n",
    "        data_dir=data_dir,\n",
    "        seq_length=512,\n",
    "        micro_batch_size=mbs,\n",
    "        global_batch_size=gbs,\n",
    "        num_workers=0,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cd07e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B\"  # 多言語対応モデル\n",
    "\n",
    "strategy = \"\"  # 分散トレーニング戦略\n",
    "max_steps = 100  # トレーニングステップ数\n",
    "accelerator = \"gpu\"\n",
    "num_devices = 1  # GPU数\n",
    "wandb_name = None  # wandb実験名\n",
    "use_torch_jit = False  # torch jit有効化\n",
    "ckpt_folder = \"/opt/checkpoints/japanese_wiki_experiments/\"  # チェックポイント保存パス\n",
    "data_dir = \"/workspace/data/ja_wiki\"  # ja_wikiデータのパス\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5c233b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_strategy(strategy, model, devices, num_nodes, adapter_only=False):\n",
    "    \"\"\"\n",
    "    分散トレーニング戦略を作成\n",
    "    \"\"\"\n",
    "    if strategy == 'ddp':\n",
    "        return pl.strategies.DDPStrategy(\n",
    "            checkpoint_io=model.make_checkpoint_io(adapter_only=adapter_only),\n",
    "        )\n",
    "    elif strategy == 'fsdp2':\n",
    "        return nl.FSDP2Strategy(\n",
    "            data_parallel_size=devices * num_nodes,\n",
    "            tensor_parallel_size=1,\n",
    "            checkpoint_io=model.make_checkpoint_io(adapter_only=adapter_only),\n",
    "        )\n",
    "    else:\n",
    "        return pl.strategies.SingleDeviceStrategy(\n",
    "            device='cuda:0',\n",
    "            checkpoint_io=model.make_checkpoint_io(adapter_only=adapter_only),\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84823b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_japanese_sft():\n",
    "    \"\"\"日本語WikipediaデータでSFTを実行\"\"\"\n",
    "\n",
    "    # WandBロガーの設定\n",
    "    wandb = WandbLogger(\n",
    "        project=\"nemo_japanese_wiki\",\n",
    "        name=wandb_name,\n",
    "    ) if wandb_name is not None else None\n",
    "\n",
    "    # コールバックの設定\n",
    "    callbacks = []\n",
    "    if use_torch_jit:\n",
    "        jit_config = JitConfig(use_torch=True, torch_kwargs={'dynamic': False}, use_thunder=False)\n",
    "        callbacks = [JitTransform(jit_config)]\n",
    "\n",
    "    callbacks.append(\n",
    "        nl.ModelCheckpoint(\n",
    "            every_n_train_steps=max_steps // 2,\n",
    "            dirpath=ckpt_folder,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # モデルの作成\n",
    "    model = llm.HFAutoModelForCausalLM(model_name=model_name)\n",
    "\n",
    "    # 戦略の作成\n",
    "    training_strategy = make_strategy(strategy, model, num_devices, 1)\n",
    "\n",
    "    # トレーナーの設定\n",
    "    trainer = nl.Trainer(\n",
    "        devices=num_devices,\n",
    "        max_steps=max_steps,\n",
    "        accelerator=accelerator,\n",
    "        strategy=training_strategy,\n",
    "        log_every_n_steps=1,\n",
    "        limit_val_batches=0.1,  # 検証データの一部のみ使用\n",
    "        num_sanity_val_steps=2,\n",
    "        accumulate_grad_batches=1,\n",
    "        gradient_clip_val=1.0,\n",
    "        use_distributed_sampler=False,\n",
    "        logger=wandb,\n",
    "        callbacks=callbacks,\n",
    "        precision=\"bf16\",\n",
    "    )\n",
    "\n",
    "    # トークナイザーの作成\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # SFTの実行\n",
    "    llm.api.finetune(\n",
    "        model=model,\n",
    "        data=japanese_wiki_data(\n",
    "            tokenizer,\n",
    "            data_dir=data_dir,\n",
    "            gbs=1\n",
    "        ),\n",
    "        trainer=trainer,\n",
    "        optim=fdl.build(llm.adam.pytorch_adam_with_flat_lr(lr=1e-5)),\n",
    "        peft=None,  # PEFTを使用する場合はここでLoRAなどを設定\n",
    "        log=None,\n",
    "    )\n",
    "\n",
    "    print(f\"Training completed! Checkpoints saved to: {ckpt_folder}\")\n",
    "    return ckpt_folder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c1558c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-14 07:49:56 nemo_logging:393] use_linear_ce_loss: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-07-14 07:49:57 nemo_logging:393] Experiments will be logged at /workspace/src/nemo_experiments/default/2025-07-14_07-47-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-07-14 07:49:57 nemo_logging:405] \"update_logger_directory\" is True. Overwriting tensorboard logger \"save_dir\" to /workspace/src/nemo_experiments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1363395 examples from 14 files\n",
      "Loaded 1134 examples from 1 files\n",
      "[NeMo I 2025-07-14 07:50:24 nemo_logging:393] Bad message (TypeError('not all arguments converted during string formatting')): {'name': 'nemo_logger', 'msg': 'Configuring model with attn_implementation:', 'args': ('sdpa',), 'levelname': 'INFO', 'levelno': 20, 'pathname': '/opt/NeMo/nemo/utils/nemo_logging.py', 'filename': 'nemo_logging.py', 'module': 'nemo_logging', 'exc_info': None, 'exc_text': None, 'stack_info': None, 'lineno': 393, 'funcName': 'info', 'created': 1752479424.0764952, 'msecs': 76.0, 'relativeCreated': 310648.4651565552, 'thread': 140005550290240, 'threadName': 'MainThread', 'processName': 'MainProcess', 'process': 248363, 'taskName': 'Task-2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type             | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model | Qwen2ForCausalLM | 494 M  | train\n",
      "---------------------------------------------------\n",
      "494 M     Trainable params\n",
      "0         Non-trainable params\n",
      "494 M     Total params\n",
      "1,976.131 Total estimated model params size (MB)\n",
      "319       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'AutoTokenizer' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPlease download ja_wiki data using the commands from the blog post.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# SFTの実行\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     checkpoint_dir = \u001b[43mrun_japanese_sft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# # 推論テスト（オプション）\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# print(\"\\n=== Inference Test ===\")\u001b[39;00m\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m#     result = pipe(\"日本の文化について教えてください。\")\u001b[39;00m\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m#     print(result)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mrun_japanese_sft\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     47\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# SFTの実行\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinetune\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjapanese_wiki_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgbs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfdl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43madam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpytorch_adam_with_flat_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpeft\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# PEFTを使用する場合はここでLoRAなどを設定\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining completed! Checkpoints saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ckpt_folder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/NeMo/nemo/collections/llm/api.py:222\u001b[39m, in \u001b[36mfinetune\u001b[39m\u001b[34m(model, data, trainer, log, resume, optim, peft)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[33;03mFinetunes a model using the specified data and trainer, with optional logging, resuming, and PEFT.\u001b[39;00m\n\u001b[32m    194\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    218\u001b[39m \u001b[33;03m    PosixPath('/path/to/log_dir')\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    221\u001b[39m _validate_config(model, data, trainer, log=log, resume=resume, optim=optim, model_transform=peft)\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_transform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpeft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/NeMo/nemo/collections/llm/api.py:127\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, data, trainer, log, resume, optim, tokenizer, model_transform)\u001b[39m\n\u001b[32m    114\u001b[39m         set_modelopt_spec_if_exists_in_ckpt(model, resume.resume_from_path)\n\u001b[32m    116\u001b[39m app_state = _setup(\n\u001b[32m    117\u001b[39m     model=model,\n\u001b[32m    118\u001b[39m     data=data,\n\u001b[32m   (...)\u001b[39m\u001b[32m    124\u001b[39m     model_transform=model_transform,\n\u001b[32m    125\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m app_state.exp_dir\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py:538\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    536\u001b[39m \u001b[38;5;28mself\u001b[39m.state.status = TrainerStatus.RUNNING\n\u001b[32m    537\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py:47\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     50\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py:574\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    568\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    569\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    570\u001b[39m     ckpt_path,\n\u001b[32m    571\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    572\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    573\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    577\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py:981\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m    978\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    979\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m    980\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m981\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    984\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m    985\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    986\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py:1023\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1024\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m   1025\u001b[39m         \u001b[38;5;28mself\u001b[39m.fit_loop.run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py:1052\u001b[39m, in \u001b[36mTrainer._run_sanity_check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1049\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_start\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1051\u001b[39m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1052\u001b[39m \u001b[43mval_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1054\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_end\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1056\u001b[39m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/utilities.py:178\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m     context_manager = torch.no_grad\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/evaluation_loop.py:128\u001b[39m, in \u001b[36m_EvaluationLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    127\u001b[39m     dataloader_iter = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     batch, batch_idx, dataloader_idx = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m previous_dataloader_idx != dataloader_idx:\n\u001b[32m    130\u001b[39m     \u001b[38;5;66;03m# the dataloader has changed, notify the logger connector\u001b[39;00m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28mself\u001b[39m._store_dataloader_outputs()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fetchers.py:133\u001b[39m, in \u001b[36m_PrefetchDataFetcher.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    130\u001b[39m         \u001b[38;5;28mself\u001b[39m.done = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batches\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done:\n\u001b[32m    132\u001b[39m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     batch = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    135\u001b[39m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/fetchers.py:60\u001b[39m, in \u001b[36m_DataFetcher.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mself\u001b[39m._start_profiler()\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28mself\u001b[39m.done = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/utilities/combined_loader.py:341\u001b[39m, in \u001b[36mCombinedLoader.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> _ITERATOR_RETURN:\n\u001b[32m    340\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     out = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._iterator, _Sequential):\n\u001b[32m    343\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/utilities/combined_loader.py:142\u001b[39m, in \u001b[36m_Sequential.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    139\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     out = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# try the next iterator\u001b[39;00m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28mself\u001b[39m._use_next_iterator()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mJapaneseWikiDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     55\u001b[39m     full_text = text\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# トークナイズ\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m encoded = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfull_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax_length\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     64\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     67\u001b[39m     \u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m: encoded[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].squeeze(),\n\u001b[32m     68\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m: encoded[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m].squeeze(),\n\u001b[32m     69\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m: encoded[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].squeeze()  \u001b[38;5;66;03m# SFTでは入力と同じ\u001b[39;00m\n\u001b[32m     70\u001b[39m }\n",
      "\u001b[31mTypeError\u001b[39m: 'AutoTokenizer' object is not callable"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # データディレクトリの確認\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Data directory {data_dir} not found!\")\n",
    "        print(\"Please download ja_wiki data using the commands from the blog post.\")\n",
    "    else:\n",
    "        # SFTの実行\n",
    "        checkpoint_dir = run_japanese_sft()\n",
    "\n",
    "        # # 推論テスト（オプション）\n",
    "        # print(\"\\n=== Inference Test ===\")\n",
    "        # from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "        # new_checkpoint = Path(checkpoint_dir) / f\"default--val_loss=0.0000-epoch=1-step={max_steps}-last/hf_weights\"\n",
    "\n",
    "        # if new_checkpoint.exists():\n",
    "        #     pipe = pipeline(\n",
    "        #         \"text-generation\",\n",
    "        #         model=AutoModelForCausalLM.from_pretrained(new_checkpoint),\n",
    "        #         tokenizer=AutoTokenizer.from_pretrained(new_checkpoint),\n",
    "        #         torch_dtype=torch.bfloat16,\n",
    "        #         device_map=\"auto\",\n",
    "        #     )\n",
    "\n",
    "        #     # 日本語でのテスト\n",
    "        #     result = pipe(\"日本の文化について教えてください。\")\n",
    "        #     print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7f526f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
